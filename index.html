<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>CaPO</title>


  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon-32x32.png">
  <!-- <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22></text></svg>"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Calibrated Multi-Preference Optimization for Aligning Diffusion Models</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://kyungmnlee.github.io">Kyungmin Lee</a><sup>1, 2</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=5aAbqZIAAAAJ&hl=en">Xiaohang Li</a><sup>3</sup>,</span>
            <span class="author-block">
              <a href="https://research.google/people/107446/?&type=google">Qifei Wang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://research.google/people/106417/?&type=google">Junfeng He</a><sup>4</sup>,</span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/junjie-ke/">Junjie Ke</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://www.irfanessa.gatech.edu/">Irfan Essa</a><sup>1,5</sup>,</span>
            <span class="author-block">
              <a href="https://alinlab.kaist.ac.kr/shin.html">Jinwoo Shin</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://sites.google.com/view/feng-yang">Feng Yang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://research.google/people/yinxiaoli/?&type=google">Yinxiao Li</a><sup>1</sup></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Google DeepMind,</span>
            <span class="author-block"><sup>2</sup>KAIST,</span>
            <span class="author-block"><sup>3</sup>Google</span>
            <span class="author-block"><sup>4</sup>Google Research</span>
            <span class="author-block"><sup>5</sup>Georgia Institute of Technology</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span style="font-weight: bold;">CVPR 2025</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2502.02588"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2502.02588"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="capo teaser">
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
    <div class="figure">
      <img src="./static/images/teaser.jpg" style="width:100%; max-width: 1280px;">
        <div class="content has-text-justified">
          <p>
          Calibrated Preference Optimization (CaPO) improves the performance of T2I diffusion models by optimizing the model with diverse reward signals. The top and bottom groups are using SDXL and SD3-medium, respectively. For each group, the first row is from base model and the second row is applying CaPO to the base model. CaPO tends to generate images of higher quality (e.g., image aestheticism, text rendering), and better prompt alignment (e.g., compositional generation), without using any human preference dataset.
          </p>
        </div>
      </div>
  </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Aligning text-to-image (T2I) diffusion models with preference optimization is valuable for human-annotated datasets,
            but the heavy cost of manual data collection limits scalability. Using reward models offers an alternative, however,
            current preference optimization methods fall short in exploiting the rich information, as they only consider pairwise
            preference distribution. Furthermore, they lack generalization to multi-preference scenarios and struggle to handle
            inconsistencies between rewards. To address this, we present Calibrated Preference Optimization (CaPO), a novel method
            to align T2I diffusion models by incorporating the general preference from multiple reward models without human annotated data. 
            The core of our approach involves a reward calibration method to approximate the general preference by computing the 
            expected win-rate against the samples generated by the pretrained models. Additionally, we propose a frontier-based pair selection method that effectively
            manages the multi-preference distribution by selecting pairs
            from Pareto frontiers. Finally, we use regression loss to
            fine-tune diffusion models to match the difference between
            calibrated rewards of a selected pair. Experimental results
            show that CaPO consistently outperforms prior methods,
            such as Direct Preference Optimization (DPO), in both single and multi-reward settings validated by evaluation on T2I
            benchmarks, including GenEval and T2I-Compbench.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column">
          <div class="content">
            <h2 class="title is-3">Proposed Method</h2>
            <img src="./static/images/method.jpg" style="width:100%; max-width: 1280px;">

            <h3 class="title is-4">(a) Synthetic data generation</h3>
            <p>
              Instead of using human-annotated preference dataset, we generate group of images from prompt, and compute reward signals for each reward models such as
              image aesthetic score (<a href="https://arxiv.org/abs/2303.14302">VILA</a>), image-text alignment score (<a href="https://arxiv.org/abs/2404.01291">VQAscore</a>), 
              and human preference score (<a href="https://arxiv.org/abs/2405.14705">MPS</a>). We find that using scaling the prompt and the number of images are crucial for performance gain,
              where we use 100K prompts from <a href="https://arxiv.org/abs/2210.14896">DiffusionDB</a> and generate 16 images per prompt.
            </p>
            <h3 class="title is-4">(b) Reward Calibration</h3>
            <p>
              Instead of using the reward model directly, we calibrate the reward model to approximate the general preference by computing the expected win-rate against the samples generated by the pretrained models.
              Specifically, we compute the approximate win-rate by using Bradley-Terry model to compute the statistics of group.
              Note that this calibration has following advantages: 
              (1) it allows us to bound the range of scores into [0,1], which helps the balanced optimization by lowering the variance within reward scores.
              (2) the calibrated rewards act as a global metric where higher calibrated reward represents better performance across different prompts, whereas the higher reward score itself does not necessarily means the better model.
            </p>
            <h3 class="title is-4">(c) Pair Selection</h3>
            <p>
              For single reward case, it is straightforward to select the pairs with the highest difference in calibrated rewards. 
              On the other hand, for multi-reward case, we need to select the pairs that are Pareto optimal in the multi-dimensional space.
              We propose a frontier-based pair selection method that effectively manages the multi-preference distribution by selecting pairs from Pareto frontiers.
              Specifically, we compute the Pareto frontiers of the calibrated rewards, and select the positives (i.e., non-dominated set) from the upper frontier and 
              negatives (i.e., dominated set) from the lower frontier. 
              After removing the potential duplicate, we randomly choose the pairs from the selected positives and negatives for each training step.
            </p>
            <h3 class="title is-4">(d) Train with CaPO Loss</h3>
            <p>
              We introduce a calibrated preference optimization (CaPO) loss to align the diffusion models with the calibrated rewards.
              Given calibrated rewards of a pair, we use regression loss to fine-tune diffusion models to match the difference between calibrated rewards of a selected pair.
              Specifically, we match the difference of implicit rewards (i.e., the log-ratio between the fine-tuning and pretrained model) to match the difference of calibrated rewards.
              Compared to <a href="https://arxiv.org/abs/2311.12908">DPO</a> and <a href="https://arxiv.org/abs/2310.12036">IPO</a>, which leverage the discrete preference label for
              training, our objective accounts the difference of calibrated rewards, which allows us to optimize the model without suffering from overfitting. 
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column">
          <h2 class="title is-3">Results</h2>
          <div class="content">
            <h3 class="title is-4">Single reward results</h3>
            <img src="./static/images/tab111.png" style="width:100%; max-width: 1280px;">
            <p>
              When using single reward model to align diffusion models (SDXL and SD3-medium), CaPO achieves the highest win-rate compared
              to the baselines DPO and IPO. Especially, when using VILA, DPO shows drop in VQAscore, while CaPO shows comparable performance 
              to the base model. This shows that CaPO is more robust to the reward model due to the reward calibration.
            </p>
          </div>
          

          <div class="content">
            <h3 class="title is-4">Multi-reward results</h3>
            <img src="./static/images/table.png" style="width:100%; max-width: 1280px;">
            <p>
              When using multiple reward models to align diffusion models (SDXL and SD3-medium), CaPO with frontier-based rejection sampling (FRS)
              achieves the highest win-rate and scores compared to other baselines. Notably, we compare with SUM, which use the sum of rewards
              as a proxy for unified reward, and SOUP, which use model soup to combine the models aligned with individual reward models.
              CaPO with FRS outperforms SUM and SOUP, showing the effectiveness of our method in handling multi-preference problem.
            </p>
          </div>


          <div class="content">
            <h3 class="title is-4">Benchmark results</h3>
            <img src="./static/images/bench.png" style="width:100%; max-width: 1280px;">
            <p>
              We evaluate the performance of CaPO on <a href="https://karine-h.github.io/T2I-CompBench/">T2I-CompBench</a> 
              and <a href="https://arxiv.org/abs/2310.11513">GenEval</a>, which are the benchmarks for T2I diffusion models.
              Note that CaPO shows significant boost in T2I-CompBench and GenEval for both SDXL and SD3-medium, respectively.
              Also, we observe that CaPO+SD3-medium shows comparable performance compared to open-source large T2I diffusion models
              such as FLUX or SD3.5-large. 
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{lee2025calibrated,
    title={Calibrated multi-preference optimization for aligning diffusion models},
    author={Lee, Kyungmin and Li, Xiahong and Wang, Qifei and He, Junfeng and Ke, Junjie and Yang, Ming-Hsuan and Essa, Irfan and Shin, Jinwoo and Yang, Feng and Li, Yinxiao},
    booktitle={Proceedings of the Computer Vision and Pattern Recognition Conference},
    year={2025}
}</code></pre>
    </div>
  </section>


<!-- <section class="section">
  <div>
    <div class="columns is-centered has-text-centered">
      <h2 class="title is-3">Results</h2>
      <p></p>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column">
        <div class="columns is-centered has-text-centered">
          <div class="column content">
            <video id="matting-video" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/bread.mp4"
                      type="video/mp4">
            </video>
            <p>
              A sliced loaf of fresh bread.
            </p>
          </div>
        </div>
      </div>

      <div class="column">
        <div class="columns is-centered has-text-centered">
          <div class="column content">
            <video id="matting-video" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/corgi_boba.mp4"
                      type="video/mp4">
            </video>
            <p>
              A corgi standing up drinking boba.
            </p>
          </div>
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column">
        <div class="columns is-centered has-text-centered">
          <div class="column content">
            <video id="matting-video" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/crown.mp4"
                      type="video/mp4">
            </video>
            <p>
              An imperial state crown of England.
            </p>
          </div>
        </div>
      </div>

      <div class="column">
        <div class="columns is-centered has-text-centered">
          <div class="column content">
            <video id="matting-video" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/garbage.mp4"
                      type="video/mp4">
            </video>
            <p>
              A beautiful dress made out of garbage bags, on a mannequin.
            </p>
          </div>
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column">
        <div class="columns is-centered has-text-centered">
          <div class="column content">
            <video id="matting-video" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/roof.mp4"
                      type="video/mp4">
            </video>
            <p>
              A 3D model of adorable cottage with a thatched roof.
            </p>
          </div>
        </div>
      </div>

      <div class="column">
        <div class="columns is-centered has-text-centered">
          <div class="column content">
            <video id="matting-video" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/silver.mp4"
                      type="video/mp4">
            </video>
            <p>
              A silver platter piled high with fruits.
            </p>
          </div>
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column">
        <div class="columns is-centered has-text-centered">
          <div class="column content">
            <video id="matting-video" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/tarantula.mp4"
                      type="video/mp4">
            </video>
            <p>
              A tarantula, highly detailed.
            </p>
          </div>
        </div>
      </div>

      <div class="column">
        <div class="columns is-centered has-text-centered">
          <div class="column content">
            <video id="matting-video" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/tiger_ice.mp4"
                      type="video/mp4">
            </video>
            <p>
              A tiger eating an ice cream cone.
            </p>
          </div>
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column">
        <div class="columns is-centered has-text-centered">
          <div class="column content">
            <video id="matting-video" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/tigerdoctor.mp4"
                      type="video/mp4">
            </video>
            <p>
              A tiger dressed as a doctor.
            </p>
          </div>
        </div>
      </div>

      <div class="column">
        <div class="columns is-centered has-text-centered">
          <div class="column content">
            <video id="matting-video" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/wedding.mp4"
                      type="video/mp4">
            </video>
            <p>
              Wedding dress made out of tenacles.
            </p>
          </div>
        </div>
      </div>
    </div>

  </div> -->

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            The template for this website is from <a href="https://github.com/nerfies/nerfies.github.io">here</a>, and we appreciate their kindness in open-sourcing it.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
