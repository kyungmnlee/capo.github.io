<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>CaPO</title>


  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon-32x32.png">
  <!-- <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22></text></svg>"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Calibrated Multi-Preference Optimization for Aligning Diffusion Models</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://kyungmnlee.github.io">Kyungmin Lee</a><sup>1, 2</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=5aAbqZIAAAAJ&hl=en">Xiaohang Li</a><sup>3</sup>,</span>
            <span class="author-block">
              <a href="https://research.google/people/107446/?&type=google">Qifei Wang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://research.google/people/106417/?&type=google">Junfeng He</a><sup>4</sup>,</span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/junjie-ke/">Junjie ke</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://www.irfanessa.gatech.edu/">Irfan Essa</a><sup>1,5</sup>,</span>
            <span class="author-block">
              <a href="https://alinlab.kaist.ac.kr/shin.html">Jinwoo Shin</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://sites.google.com/view/feng-yang">Feng Yang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://research.google/people/yinxiaoli/?&type=google">Yinxiao Li</a><sup>1</sup></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Google DeepMind,</span>
            <span class="author-block"><sup>2</sup>KAIST,</span>
            <span class="author-block"><sup>3</sup>Google</span>
            <span class="author-block"><sup>4</sup>Google Research</span>
            <span class="author-block"><sup>5</sup>Georgia Institute of Technology</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span style="font-weight: bold;">CVPR 2025</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2502.02588"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2502.02588"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="capo teaser">
  <div class="columns is-centered has-text-centered">
    <div class="mySlides">
      <img src="./static/images/teaser.jpg" style="width:100%; max-width: 1280px;">
      <div class="imagetext">
        Teaser
      </div>
    <!-- </div>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
        free-viewpoint
        portraits.
      </h2>
    </div> -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Aligning text-to-image (T2I) diffusion models with preference optimization is valuable for human-annotated datasets,
            but the heavy cost of manual data collection limits scalability. Using reward models offers an alternative, however,
            current preference optimization methods fall short in exploiting the rich information, as they only consider pairwise
            preference distribution. Furthermore, they lack generalization to multi-preference scenarios and struggle to handle
            inconsistencies between rewards. To address this, we present Calibrated Preference Optimization (CaPO), a novel method
            to align T2I diffusion models by incorporating the general preference from multiple reward models without human annotated data. 
            The core of our approach involves a reward calibration method to approximate the general preference by computing the 
            expected win-rate against the samples generated by the pretrained models. Additionally, we propose a frontier-based pair selection method that effectively
            manages the multi-preference distribution by selecting pairs
            from Pareto frontiers. Finally, we use regression loss to
            fine-tune diffusion models to match the difference between
            calibrated rewards of a selected pair. Experimental results
            show that CaPO consistently outperforms prior methods,
            such as Direct Preference Optimization (DPO), in both single and multi-reward settings validated by evaluation on T2I
            benchmarks, including GenEval and T2I-Compbench.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </section>
  
  <!-- <section class="section">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Approximating Probability Flow ODE</h2>
        <div class="mySlides">
          <img src="./static/Figure_2.jpg" style="width:100%; max-width: 1280px;">
          <div class="imagetext">
            Our method, approximate probability flow ODE (APFO), uses predetermined timestep schedule in contrast to score distillation sampling (SDS),
            and amortize optimization to update multi-view images in 3D model. We fine-tune denoiser to accurately compute the probability flow. 
            </div>
        </div>
        <br>
        <div class="content has-text-justified">
          <p>
            We introduce approximate probability flow ODE (APFO), which approximates the probability flow to update 3D model.
            Given an image rendered from a 3D model, we optimize the image by transporting it to the high-density region of text-to-image
            diffusion model using Schrodinger bridge. We model the score function of current scene by additional fine-tuning, and 
            compute the approximate probability flow by the different of the denoiser outputs of pretrained model and fine-tuned model.
            Alike conventional diffusion sampler, we use decreasing sequence of timesteps to guarantee convergence, and amortize the optimization
            to optimize the multi-view images in a 3D scene. 
          </p>
        </div>
      </div>
    </div>

  </div>
</section>
<section class="section">
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3">Coarse-to-fine text-to-3D optimization of DreamFlow</h2>
      <div class="mySlides">
        <img src="./static/Figure_2_2.jpg" style="width:100%; max-width: 1280px;">
        <div class="imagetext">
          Our text-to-3D generation is done in coarse-to-fine manner; we first optimize NeRF, then extract 3D mesh and fine-tune. 
          We use same latent diffusion model (denoiser 1) for first and second stage. Lastly, we refine 3D mesh with high-resolution 
          latent diffusion prior (denoiser 2). At each stage, we optimize with different timestep schedule, which effectively utilize the diffusion priors.
          </div>
      </div>
      <br>
      <div class="content has-text-justified">
        <p>
          The proposed framework, DreamFlow, perform coarse-to-fine text-to-3D optimization for high-quality 3D content generation.
          We first optimize NeRF (e.g., using hash-grid encoder) using latent diffusion model (e.g., Stable Diffusion v2.1) with resolution of 256x256, 
          with timesteps decreasing from 1.0 to 0.2. Then, we extract a 3D mesh from stage 1 for efficient 3D modeling, and optimize 3D mesh with resolution
          of 512x512 using same denoiser of stage 1, with timesteps decreasing from 0.5 to 0.1. Lastly at stage 3, we refine the 3D mesh using diffusion refiner 
          (e.g., Stable Diffusion XL refiner), to generate 3D mesh in resolution of 1024x1024. 3D mesh refinement significantly enhance the photorealism of 3D 
          model, compared to prior methods. 
        </p>
      </div>
    </div>
  </div>
</div>
</section> -->


<!-- <section class="section">
  <div>
    <div class="columns is-centered has-text-centered">
      <h2 class="title is-3">Results</h2>
      <p></p>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column">
        <div class="columns is-centered has-text-centered">
          <div class="column content">
            <video id="matting-video" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/bread.mp4"
                      type="video/mp4">
            </video>
            <p>
              A sliced loaf of fresh bread.
            </p>
          </div>
        </div>
      </div>

      <div class="column">
        <div class="columns is-centered has-text-centered">
          <div class="column content">
            <video id="matting-video" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/corgi_boba.mp4"
                      type="video/mp4">
            </video>
            <p>
              A corgi standing up drinking boba.
            </p>
          </div>
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column">
        <div class="columns is-centered has-text-centered">
          <div class="column content">
            <video id="matting-video" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/crown.mp4"
                      type="video/mp4">
            </video>
            <p>
              An imperial state crown of England.
            </p>
          </div>
        </div>
      </div>

      <div class="column">
        <div class="columns is-centered has-text-centered">
          <div class="column content">
            <video id="matting-video" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/garbage.mp4"
                      type="video/mp4">
            </video>
            <p>
              A beautiful dress made out of garbage bags, on a mannequin.
            </p>
          </div>
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column">
        <div class="columns is-centered has-text-centered">
          <div class="column content">
            <video id="matting-video" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/roof.mp4"
                      type="video/mp4">
            </video>
            <p>
              A 3D model of adorable cottage with a thatched roof.
            </p>
          </div>
        </div>
      </div>

      <div class="column">
        <div class="columns is-centered has-text-centered">
          <div class="column content">
            <video id="matting-video" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/silver.mp4"
                      type="video/mp4">
            </video>
            <p>
              A silver platter piled high with fruits.
            </p>
          </div>
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column">
        <div class="columns is-centered has-text-centered">
          <div class="column content">
            <video id="matting-video" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/tarantula.mp4"
                      type="video/mp4">
            </video>
            <p>
              A tarantula, highly detailed.
            </p>
          </div>
        </div>
      </div>

      <div class="column">
        <div class="columns is-centered has-text-centered">
          <div class="column content">
            <video id="matting-video" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/tiger_ice.mp4"
                      type="video/mp4">
            </video>
            <p>
              A tiger eating an ice cream cone.
            </p>
          </div>
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column">
        <div class="columns is-centered has-text-centered">
          <div class="column content">
            <video id="matting-video" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/tigerdoctor.mp4"
                      type="video/mp4">
            </video>
            <p>
              A tiger dressed as a doctor.
            </p>
          </div>
        </div>
      </div>

      <div class="column">
        <div class="columns is-centered has-text-centered">
          <div class="column content">
            <video id="matting-video" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/wedding.mp4"
                      type="video/mp4">
            </video>
            <p>
              Wedding dress made out of tenacles.
            </p>
          </div>
        </div>
      </div>
    </div>

  </div> -->

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            The template for this website is from <a href="https://github.com/nerfies/nerfies.github.io">here</a>, and we appreciate their kindness in open-sourcing it.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
